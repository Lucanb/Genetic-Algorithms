\documentclass{article}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{ragged2e,booktabs,caption}
\usepackage[margin=25mm]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{ltablex}
\let\elskeyword\keyword
\let\endelskeyword\endkeyword
\begin{Document}

\title{A comparison between Genetic Algorithm and Simulated Annealing using Travelling Salesman Problem Instances \\
\vspace{10mm}
\large Alexandru Ioan Cuza University - Department of Computer Science}
\author{Luca Năstasă Baraș,Dragoș Damian}
\maketitle

\begin{abstract}
Travelling salesman problem(TSP) is a NP-hard problem in combinatorial optimisation and in this report we want to do a comparison between Simulating Annealing and a Genetic Algorithm, both metaheuristic methods, on 10 instances of our optimisation problem. 
\end{abstract}


\textbf{Keywords:} Genetic Algorithm, Simulated Annealing

\section{Introduction}
The travelling salesman problem (TSP) is a combinatorial optimisation problem  that is derived from many production and scheduling problems and is presented
as a salesman being required to visit n cities but given the traveling costs between each city, the salesman has to find the shortest distance required to visit all required cities.
\par In this paper we will try to approach the problem optima with a genetic algorithm(GA) and with a trajectory method Simulating Annealing(SA). When we will do a comparison between both we will see that GA has better results as the number of the cities is increasing.
\par The test problems I used consists in 10 distinct instances and the difficulty of those is increasing from the first one to the last(the number of the cities of every instance represents the difficulty). Our chosen instances are : bayg29, berlin52, bier127, d198, gil262, pcb442, pa561, rl1889,  rl5915, d15112   
\par The implemented SA algorithm starts with a temperature of 100 and it is cooling to $10^{-3}$. We chose a medium cooling function because there are many instances with big dimension differences and we chose a result in a reasonable time even if with a slower cooling function we could achieve better results.
\par The GA is projected as follows: the genotype represents the order in which a salesman visits the cities and the mutation and crossover must ensure that no city is visited more than once (We placed the restrictions on our used techniques as selection, mutation and crossover). So our GA follows the classic schema with some optimisations added for better results.The dimension of population is 200 individuals and the number of generations is 2000. We tried both Tournament and Wheel Of Fortune selections and  we chose the Tournament one .
\newpage
\par And as mutations we tried for :SBM, Slide Mutation, IRGIBNM, RGIBNM and in the end we chose the (Selection Best Mutation)SBM one which is a greedy method of choosing the best mutation and it is shown to be more stable , it is made from IRGIBNM mutation ,RGIBNM mutation Slide mutation and Inversion mutation. The chosen cross-over between (Order Cross Over) OX, (Cycle Cross Over)CX and GX is GX one, that has a greedy strategy too, it is proposed by Grefenstette in 1984.  

\section{General Setting}

\subsection{Genetic Algorithm}
In this following pseudo code is the general schema of our genetic algorithm :
\vspace{3mm} %10mm vertical space

{
\centering
\begin{algorithmic}
\State CreatePopulation$\left(population\right);$ \Comment{We generate a random population}
\State evaluate$\left(population\right)$
\State $i \gets 1$;
\While{i $ \; \leq \; $ Number of Generations }
    \State select$\left(population\right)$
    \State mutate$\left(population\right)$
    \State crossover$\left(population\right)$
    \State evaluate$\left(population\right)$
    \State $i \gets i + 1$
\EndWhile

\State \Return best$\left(pop\right)$

\newline
\par The chromosome is represented by a route of n cities (all cities are included and exclusive) so it can be consider a permutation(values are $1->n$).
Our population of candidate solutions are generated by shuffling the permutation and then for finding the optimum we apply the genetic operators : selection mutation and cross over. 
\par Our Fitness formula is : $$fit = \frac{f_i}{\sum_{i=1}^n f_{i}} \; $$

\par As selection method we chose the Tournament one because we observed in our process that if we use the Wheel of Fortune, we have to deal with premature convergence and
we can control the selection pressure better. The Tournament selection randomly chooses k elements$(1<k<n)$ (from a population) that represents our pool and then from those k elements we choose the best element and we put it in our new population.

\par The cross over operation is done with one cutting point where each child replaces its parents. The generation
is sorted ascending by its own crossover probability (that is generated randomly at the initialization step) and
the cutting point is decided when first individual has a better crossover probability than the global crossover probability. In our algorithm we used GX Cross Over that uses an heuristic approach invented by Grefenstette  : 
\begin{enumerate}
    \item randomly select the first city from our parent 
    \item find the city that follows right after selected city in both parents
    \item select the city which yields the lowest cost
    \item append the city to the new route list
    \item select the appended city from the new route list as template
    \item repeat the process
\end{enumerate}
\par We used elitism in proportion of $12\%$(replacing the best individuals from population) and the cross-over probability is $4 \cdot 10^{-1}$. 
\par For better precision we chose to first use a small space search for every individual of our population and start the GA from the better point. The search is done with a selection pressure and the neighbours are chosen after the reverse mutation. 
\par The mutation operator is made after BMS(Best mutation Selection) strategy and it has to choose the best mutation between : IRGIBNM, RGIBNM, Slide Mutation and Inversion Mutation(randomly choose 2 position in our route and then reverse the city order between those 2 position).
\par \textbf{IRGIBNM is a new hybrid mutation that :}
\begin{enumerate}
    \item randomly chooses 2 position of our route list and reverse the cities order from that sequence.
    \item uses RGIBNM mutation on the new genotype, which says that after we chose a random city on position k from our route list,we look for the closest city of our random chosen city on position p,after that we chose another city near the city on the p position which is on position q and then swapping the city on k+1 position(neighbour of our random chosen city) with the city on the position q. 
\end{enumerate}

\par The mutation happens when the
mutation probability, which is set to 0.02, is higher than a randomly generated number between 0 and 1.
\end{algorithmic}
}
\vspace{2mm} %10mm vertical space

\subsection{Simulating Annealing}
In this following pseudo code is the general schema of our SA :
\vspace{3mm} %10mm vertical space
{
\centering
\begin{algorithmic}
\State $iter \gets 100$; \;\; 
\While{$iterations \geq 1$}
\State $route \gets rangomPerm\left(\left\{1, 2, \dots, k\right\}\right)$  
\State $t \gets 100$; \;\; 
\State $CostOfRoute \gets 0$; $neighbour \gets 0$
\State $CostOfRoute \gets Cost\left(route\right)$
\State $best \gets 1000000$
\While{t $ \; \geq \; $ 0.001 }
    \State $neighbour\gets route$
    \State $RandomSwap(neighbour)$
    \State $NeighbourCost \gets Cost\left(neighbour\right)$
    \If{$Neighbourcost < CostOfRoute$}
        \State $route \gets neighbour$
        \State $CostOfRoute \gets Neighbuorcost$
    \Else
         \If{$random\left(0, 0.999999\right) < exp\left(-abs\left(CostOfRoute - Neighbourcost\right)/t\right)$} 
            \State $route \gets neighbour$
            \State $CostOfRoute \gets NeighbourCost$
            \State $t \gets t \cdot 0.995$
    \EndIf
    \EndIf
\EndWhile
\State $best \gets min\left(bestCost, CostOfRoute\right)$
\State $i \gets i - 1$
\EndWhile
\State \Return best
\end{algorithmic}
}

\section{Results}
\par \textbf{This are or results for both algorithms : }
\begin{center}
  \begin{tabular}{|p{2.3cm}|p{2.5cm}||p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|} 
    \hline
    \multicolumn{6}{|c|}{Simulated Annealing} \\
    \hline
    Instance      &  Optimal cost  & Best Cost &  Mean & StDev & Duration \\ 
    \hline\hline
    bayg29          & $1610$ & $ 8443 $ & $ 8872 $ & $ 224 $ & 7min 43s \\ 
    \hline
    berlin52         & $ 7542 $ & $ 8250 $ & $ 8719.3 $ & $254.1198$ & 3min 15s \\ 
    \hline
    bier127          & $118282$ & $ 154329 $ & $ 163216.6 $ & $ 3029.44$ & 4min 25s \\ 
    \hline
    d198          & $15780$ & $ 41762 $ & $ 52251 $ & $ 9609 $ & 2min 07s \\ 
    \hline
    gil262         & $2378$ & $ 7632 $ & $ 9243 $ & $444.0558 $ & 57s \\ 
    \hline
    pcb442        & $50778$ & $ 201420 $ & $ 212654 $ & $ 6874 $ & 5min 37s \\ 
    \hline
    pa561        & $2763$ & $ 72031 $ & $ 73041.7 $ & $ 2623 $ & 2min 54s \\ 
    \hline
    rl1889        & $316536$ & $ 4.171830e+06 $ & $ 4.532114e+06 $ & $ 1.343237e+05 $ & 23m 04s \\ 
    \hline
    rl5915        & $565530$ & $ 1.382040e+07 $ & $ 1.576210e+07 $ & $ 1.420373e+06 $ & 1h36m51s\\ 
    \hline
    d15112        & $1.573084e+06$ & $ 7.471423e+07 $ & $ 8.002034e+07 $ & $ 1.871274e+06 $ & 1h 27m \\ 
    \hline
 \end{tabular}
\end{center}


\begin{center}
  \begin{tabular}{|p{2.3cm}|p{2.5cm}||p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2cm}|} 
    \hline
    \multicolumn{6}{|c|}{Genetic Algorithm} \\
    \hline
    Instance      &  Optimal cost  & Best Cost &  Mean & StDev & Duration \\ 
    \hline\hline
    bayg29          & $1610$ & $ 1610 $ & $ 1749 $ & $ 24 $ & 1m24s \\ 
    \hline
    berlin52         & $ 7542 $ & $ 7542 $ & $ 7640 $ & $234$ & 4min 35s \\ 
    \hline
    bier127          & $118282$ & $ 120526 $ & $ 130973.1 $ & $ 3192$ & 4min 24s \\ 
    \hline
    d198          & $15780$ & $ 15783 $ & $ 16896 $ & $ 263 $ & 5min 16s \\ 
    \hline
    gil262         & $2378$ & $ 2378 $ & $ 2547 $ & $87$ & 6min 47s \\ 
    \hline
    pcb442        & $50778$ & $ 56432 $ & $ 60213 $ & $ 4520 $ & 26min 06s \\ 
    \hline
    pa561        & $2763$ & $ 28436 $ & $ 3276.7 $ & $ 4163.0741 $ & 17min 57s \\ 
    \hline
    rl1889        & $316536$ & $ 665423 $ & $ 865423 $ & $ 342567 $ & 1h 24m \\ 
    \hline
    rl5915        & $565530$ & $ 764522 $ & $ 864520 $ & $ 123421 $ & 2h 17m \\ 
    \hline
    d15112        & $1.573084e+06$ & $ 2.272324e+07  $ & $ 2.872321e+07 $ & $ 689231 $ & 7h42m \\ 
    \hline
 \end{tabular}
\end{center}

\subsection{Interpretation}
\par For the genetic algorithm we see that it has better results on 10 instances. As we can see, as the instance gets harder, it has more cities and the Genetic algorithm is farther from the optimum.
\par The time complexity for GAs is increasing as the instances are harder but for simulating annealing this isn't happening because of the cooling function.For instance the exponential expression is very small so the probability to generate a smaller number than that expression is very low and our SA starts to have a greedy strategy(choosing the best next solution).
\newpage
\par \textbf{Here we have a graph of evolution for SA :}

\includegraphics[scale=0.65]{SA52Berlin.png}

\par We can see in the figure that our SA starts to converge much faster to optimum but at iteration 5000 it starts to be locked on the plateau with the approximate value 10000.

\par \textbf{Here we have a graph of evolution for out Genetic Algorithm :}

\includegraphics[scale=0.65]{FIgure_GA52berlin.png}

\par We can say about the upper figure that the GA converges slower than SA to the optimum but it start to reach some plateaus and  pass them until a close solutions to the optimum.

\par \textbf{Here we can see the values on some instances for both algorithms :}

\includegraphics[scale=0.65]{SAvsGenetic.png}

\subsection{Comparison Between Mutation Strategies}

\par \textbf{Our mutation strategies are : Slide Inversion RGIBNM IRGIBNM SBM}


\includegraphics[scale=0.8]{MutationComparison.png}


\par As we see in the upper graphs, the best convergence for some instances is for SBM and some mutation are better than others in some cases but as a media SBM looks to be more stable (its errors are not so big). 

\subsection{Comparison Between Cross Over Strategies}

\includegraphics[scale=0.65]{Cross Comparison.png}

\
\par In this picture we can see that for the majority of cases, in our problem GX Cross Over converges better than others to optimum cost. 

\section{Conclusion}
The Genetic Algorithms are very powerful in optimisation problems like Travelling Salesman and for some instances they can find even the optimum in a polynomial time. Our GAs have different strategies, so we can say that they are very diverse and they can be modeled for every instance having a better precision. 
\par In this report we have done a comparison between SA and GA and we can say that SA is powerful especially on small instances but in the evolutive process it has a premature convergence and it is difficult to escape the plateaus. But the GA with some different optimisations can converge far difficult than SA and it can escape the plateaus, improving better over time.
\par As a conclusion, we must choose every algorithm after the nature of every instance because SA can work better on some instances than GA but overall GA looks to have better results on more instances than SA.

\begin{thebibliography}{7}
    \bibitem{Grefenstette} Grefenstette J.J. (Ed.) - \textit{Proc. of the First Int. Conf. on Genetic Algorithms}, Lawrence Erlbaum Associates, Hillsdale, NJ, 1982    
    \bibitem{course} Seminar Genetic Algorithms, author Eugen Croitoru
  \url{https://profs.info.uaic.ro/~eugennc/teaching/ga/}.
\end{thebibliography}

\end{document}